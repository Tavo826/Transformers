{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers",
      "provenance": [],
      "authorship_tag": "ABX9TyO2xeA9ZiLgGufIFNQveOCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tavo826/Transformers/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZi_bhwNNrYO"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context = \"talk\")\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMwT_sJR8DdD"
      },
      "source": [
        "## Arquitectura del modelo\n",
        "\n",
        "**Encoder-decoder**\n",
        "\n",
        "El encoder mapea una secuencia de entrada de representaciones de símbolos ($x_1$,...,$x_n$) a una secuencia de representaciones continua Z = ($z_1$,...,$z_n$). Con esta Z, el decoder genera una secuencia de salida ($y_1$,...,$y_m$) de símbolos, un elemento a la vez. En cada paso el modelo es auto-regresivo, tomando los símbolos generados previamente como una entrada adicional cuando se generra la siguiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtETv7tg7I6d"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = EncoderDecoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "    #Toma y procesa las secuencias masked src y target\n",
        "    return self.decode(self.encode(src, src_mask), src_mask, \n",
        "                       tgt, tgt_mask)\n",
        "    \n",
        "  def encode(self, src, src_mask):\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6AwPAN_3IH"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  #Define el paso de generacion linear + softmax\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Generator, self).__init__()\n",
        "    solf.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kkczOruDHxV"
      },
      "source": [
        "## Encoder and Decoder Stacks\n",
        "\n",
        "**Encoder**\n",
        "\n",
        "El encoder se compone de un apilamiento de N=6 capas idénticas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6yfMduMDFMb"
      },
      "source": [
        "def clones(module, N):\n",
        "  #Produce N capas idénticas\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vks098NRInBi"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    #Pasa la entrada (y mask) a través de cada capa por turno\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXCZ3oqvJl9i"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = s.std(-1, keepdim=True)\n",
        "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN2D2sv6LbP0"
      },
      "source": [
        "La salida de cada subcapa es LayerNorm(x + Sublayer(x))\n",
        "\n",
        "Cada subcapa, y las capas embebidas, producen una salida de dimensión ${d}_{model}$ = 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kZ43mxuL2PS"
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  #Una capa residual seguida por una capa normal\n",
        "  def __init__(self, size, dropout):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    #Aplica una conexión residual a cada subcapa con el mismo tamaño\n",
        "    return x + self.dropuout(subllayer(self.norm(x)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI_p2Fh2NwqP"
      },
      "source": [
        "Cada capa tiene dos subcapas, la primera es una multi-head self-attention mechanism, y la segunda es una position-wise fully connected feed-forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJk27MfuP2b7"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerCConection(size, dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhZ4F5dCSRPJ"
      },
      "source": [
        "**Decoder**\n",
        "\n",
        "El decoder también se compone de N=6 capas idénticas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h12Nz_DqSZwb"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  #Capa N genérica con mask\n",
        "  def __init__(self, layer, N):\n",
        "    super(Decoder, self).__init__()\n",
        "    self. layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, memory, src_mas, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgdrQCy2TVwd"
      },
      "source": [
        "Además de las dos subcapas en cada capa del encoder, el decoder agrega una tercera capa, la cual realiza multi-head attention sobre la salida del encoder stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsNuV2rgTVc7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}